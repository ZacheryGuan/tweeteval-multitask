{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27vtDFBzOvhf"
   },
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2JIi_H4qN2QB",
    "outputId": "177000c6-4900-4375-e9ed-7bfb592aff02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: git: command not found\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f83eba137f0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/fasttext/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f83eb9d5070>: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/fasttext/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f83eba4cd60>: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/fasttext/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f83ebaace80>: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/fasttext/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f83eb9864f0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/fasttext/\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement fasttext\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for fasttext\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# download dataset\n",
    "!git clone https://github.com/cardiffnlp/tweeteval.git\n",
    "# install requirements\n",
    "!pip install fasttext torch==1.6.0 transformers torchtext==0.2.3 nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42EE4tZVR4qL",
    "outputId": "ced3c7a7-458b-49b1-f71e-51042eb9aff1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-26 06:46:22--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... failed: Name or service not known.\n",
      "wget: unable to resolve host address 'dl.fbaipublicfiles.com'\n",
      "gzip: cc.en.300.bin.gz: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# better to save in google drive and mount\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
    "!gzip -d cc.en.300.bin.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rB3VgL3_O3GT"
   },
   "source": [
    "# Process data and create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TUWuf4v2QKMn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import fasttext\n",
    "from tqdm import tqdm\n",
    "from math import log\n",
    "from itertools import chain\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cuda + torch + GPU compability\n",
    "a = torch.arange(256).reshape((16, 16)).to(device, dtype=torch.float)\n",
    "l = nn.Linear(16, 2).to(device)\n",
    "a = l(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9Z4YPjCOzMQ",
    "outputId": "746f79df-95fc-43cb-a56d-b2abfc25850d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft = fasttext.load_model('cc.en.300.bin')  # may use 1 min\n",
    "# ft = fasttext.load_model('/content/drive/MyDrive/cc.en.300.bin')\n",
    "ft.get_word_vector('hello').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "K9PSUHItUY58"
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "task = \"offensive\"\n",
    "with open(f\"tweeteval/datasets/{task}/train_text.txt\") as xs,\\\n",
    "open(f\"tweeteval/datasets/{task}/train_labels.txt\") as ys:\n",
    "    for y, x in zip(ys.readlines(), xs.readlines()):\n",
    "        train_data.append((int(y.strip()), x.strip().split(' ')))\n",
    "\n",
    "val_data = []\n",
    "with open(f\"tweeteval/datasets/{task}/val_text.txt\") as xs,\\\n",
    "open(f\"tweeteval/datasets/{task}/val_labels.txt\") as ys:\n",
    "    for y, x in zip(ys.readlines(), xs.readlines()):\n",
    "        val_data.append((int(y.strip()), x.strip().split(' ')))\n",
    "\n",
    "test_data = []\n",
    "with open(f\"tweeteval/datasets/{task}/test_text.txt\") as xs,\\\n",
    "open(f\"tweeteval/datasets/{task}/test_labels.txt\") as ys:\n",
    "    for y, x in zip(ys.readlines(), xs.readlines()):\n",
    "        test_data.append((int(y.strip()), x.strip().split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DYdNtK-vVBRs",
    "outputId": "b8a8782f-d45b-429a-bf4f-dd1152f0fc02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  ['@user',\n",
       "   'Bono...',\n",
       "   'who',\n",
       "   'cares.',\n",
       "   'Soon',\n",
       "   'people',\n",
       "   'will',\n",
       "   'understand',\n",
       "   'that',\n",
       "   'they',\n",
       "   'gain',\n",
       "   'nothing',\n",
       "   'from',\n",
       "   'following',\n",
       "   'a',\n",
       "   'phony',\n",
       "   'celebrity.',\n",
       "   'Become',\n",
       "   'a',\n",
       "   'Leader',\n",
       "   'of',\n",
       "   'your',\n",
       "   'people',\n",
       "   'instead',\n",
       "   'or',\n",
       "   'help',\n",
       "   'and',\n",
       "   'support',\n",
       "   'your',\n",
       "   'fellow',\n",
       "   'countrymen.']),\n",
       " (1,\n",
       "  ['@user',\n",
       "   'Eight',\n",
       "   'years',\n",
       "   'the',\n",
       "   'republicans',\n",
       "   'denied',\n",
       "   'obamaâ€™s',\n",
       "   'picks.',\n",
       "   'Breitbarters',\n",
       "   'outrage',\n",
       "   'is',\n",
       "   'as',\n",
       "   'phony',\n",
       "   'as',\n",
       "   'their',\n",
       "   'fake',\n",
       "   'president.']),\n",
       " (0,\n",
       "  ['@user',\n",
       "   'Get',\n",
       "   'him',\n",
       "   'some',\n",
       "   'line',\n",
       "   'help.',\n",
       "   'He',\n",
       "   'is',\n",
       "   'gonna',\n",
       "   'be',\n",
       "   'just',\n",
       "   'fine.',\n",
       "   'As',\n",
       "   'the',\n",
       "   'game',\n",
       "   'went',\n",
       "   'on',\n",
       "   'you',\n",
       "   'could',\n",
       "   'see',\n",
       "   'him',\n",
       "   'progressing',\n",
       "   'more',\n",
       "   'with',\n",
       "   'his',\n",
       "   'reads.',\n",
       "   'He',\n",
       "   'brought',\n",
       "   'what',\n",
       "   'has',\n",
       "   'been',\n",
       "   'missing.',\n",
       "   'The',\n",
       "   'deep',\n",
       "   'ball',\n",
       "   'presence.',\n",
       "   'Now',\n",
       "   'he',\n",
       "   'just',\n",
       "   'needs',\n",
       "   'a',\n",
       "   'little',\n",
       "   'more',\n",
       "   'time'])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "for _, ws in train_data:\n",
    "    vocabulary |= set(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36465\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocabulary) + 1\n",
    "tkn2idx = {w: i for i, w in enumerate(vocabulary, start=1)}\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "SYGGBt4jTMPO"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "PAD_IDX = 0\n",
    "ft_text_pipeline = lambda words:np.array([ft.get_word_vector(word) for word in words if word != \"@user\"])\n",
    "tkn_text_pipeline = lambda words:[tkn2idx[word] for word in words if word in vocabulary]\n",
    "def collate_batch(batch):\n",
    "    '''\n",
    "    input: List[(label, sentence)]\n",
    "    '''\n",
    "    label_list, text_list, seq_len = [], [], []\n",
    "    for label, words in batch:\n",
    "        label_list.append(label)\n",
    "        processed_text = torch.tensor(tkn_text_pipeline(words), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        seq_len.append(processed_text.shape[0])\n",
    "    # print(label_list, text_list, seq_len)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    # print(len(text_list))\n",
    "    text_list = pad_sequence(text_list, padding_value=PAD_IDX)\n",
    "    # print(text_list.shape)\n",
    "    seq_len = torch.tensor(seq_len, dtype=torch.int64)\n",
    "    return label_list.to(device), text_list.to(device), seq_len.to(device)\n",
    "\n",
    "def collate_ft(batch):\n",
    "    '''\n",
    "    input: List[(label, sentence)]\n",
    "    '''\n",
    "    label_list, text_list, seq_len = [], [], []\n",
    "    for label, words in batch:\n",
    "        label_list.append(label)\n",
    "\n",
    "        # use fasttext embedding\n",
    "        processed_text = torch.tensor(ft_text_pipeline(words), dtype=torch.float)\n",
    "        # print(\"processed_text.shape,\", processed_text.shape)\n",
    "        text_list.append(processed_text)\n",
    "        seq_len.append(processed_text.shape[0])\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list, padding_value=0.)\n",
    "    seq_len = torch.tensor(seq_len, dtype=torch.int64)\n",
    "    return label_list.to(device), text_list.to(device), seq_len.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_text.shape, torch.Size([31])\n",
      "processed_text.shape, torch.Size([17])\n",
      "processed_text.shape, torch.Size([44])\n",
      "text_list.shape torch.Size([44, 3])\n"
     ]
    }
   ],
   "source": [
    "# test data process procedure\n",
    "ttt = []\n",
    "for l, w in train_data[:3]:\n",
    "    processed_text = torch.tensor(tkn_text_pipeline(w), dtype=torch.int64)\n",
    "    print(\"processed_text.shape,\", processed_text.shape)\n",
    "    ttt.append(processed_text)\n",
    "text_list_tmp = pad_sequence(ttt, padding_value=PAD_IDX)\n",
    "print(\"text_list.shape\", text_list_tmp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7NO35rySV7A"
   },
   "source": [
    "Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "MIJqDF14SVsG"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim: int, drop_rate=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-log(10000.0) / embed_dim))\n",
    "        pe = torch.zeros(1, max_len, embed_dim)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "#         print(\"PositionalEncoding input x shape\", x.shape, \"self.pe[:, :x.size(1)]\", self.pe[:, :x.size(1)].shape)\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = PositionalEncoding(embed_dim)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # todo\n",
    "        inputs = self.token_emb(inputs)\n",
    "        inputs = self.pos_emb(inputs)  # in, out B T hidden\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "5H4UmYvgQlRw"
   },
   "outputs": [],
   "source": [
    "class TransformerClassfication(nn.Module):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, n_heads, attn_drop_rate, layer_drop_rate, dense_dim):\n",
    "        super().__init__()\n",
    "        # self.emb = PositionalEncoding(embed_dim)\n",
    "        self.emb = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "\n",
    "        self.transformer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=n_heads, dim_feedforward=4*embed_dim, activation=\"gelu\")\n",
    "        # self.transformer = TransformerBlock(embed_dim, n_heads, attn_drop_rate, layer_drop_rate)\n",
    "#         self.pool = nn.AvgPool1d(kernel_size=embed_dim)\n",
    "        self.d1 = nn.Dropout(layer_drop_rate)\n",
    "        self.fc1 = nn.Linear(embed_dim, dense_dim)\n",
    "        self.act1 = nn.Sequential(nn.ReLU(), nn.Dropout(layer_drop_rate))\n",
    "        self.fc2 = nn.Linear(dense_dim, 2)\n",
    "#         self.out = nn.Softmax() # included in loss\n",
    "        \n",
    "    def init_weight(self):\n",
    "        nn.init.xavier_uniform_(self.fc1)\n",
    "        nn.init.xavier_uniform_(self.fc2)\n",
    "        nn.init.uniform_(self.emb)\n",
    "    \n",
    "    def forward(self, x, seq_len):\n",
    "        '''\n",
    "        x: S, B, embed_dim\n",
    "        '''\n",
    "        x = self.emb(x.transpose(0,1))\n",
    "        x = self.transformer(x)  # B, S, H\n",
    "        # print(\"after transformer shape,\", x.shape)\n",
    "        # after transformer shape, torch.Size([32, 38, 64])\n",
    "        \n",
    "        # take average along seq l\n",
    "        masks = (torch.arange(x.shape[1], device=device)[None, :] >= seq_len[:, None]).to(device)\n",
    "        masked_x = x.transpose(1,2).masked_fill(masks[:, None]==1, 0)  # 4, 8, 5 B, H, SEQL\n",
    "        # hidden =8, after average = 8; seql =5, after avg = 1\n",
    "        avg_t = torch.sum(masked_x, dim=2, dtype=torch.float) / seq_len[:, None] # 4, 8, 1 / 4, 1\n",
    "        x = avg_t.to(device)\n",
    "#         print(\"after pool shape,\", x.shape)  # B, H\n",
    "        \n",
    "        x = self.d1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FTTransformerClassfication(TransformerClassfication):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, n_heads, attn_drop_rate, layer_drop_rate, dense_dim):\n",
    "        super().__init__(maxlen, vocab_size, embed_dim, n_heads, attn_drop_rate, layer_drop_rate, dense_dim)\n",
    "        self.emb = PositionalEncoding(embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "Fghq64XKY_2t"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader, log_interval=5):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, seq_len) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, seq_len)\n",
    "        # print(\"predcited\", predicted_label)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        # loss.requres_grad = True\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| loss {:5.3f} '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              loss.item(),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, seq_len) in enumerate(dataloader):\n",
    "            predicted_label = model(text, seq_len)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rEhD_Cg6bCz0",
    "outputId": "b8e4f007-e551-4199-bfd5-88a8a02e769f"
   },
   "outputs": [],
   "source": [
    "# use embedding layer\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 128\n",
    "EMBED_DIM = 128\n",
    "N_HEADS = 2\n",
    "DENSE_DIM = 32\n",
    "\n",
    "model = TransformerClassfication(maxlen=5000, vocab_size=vocab_size, embed_dim=EMBED_DIM, \n",
    "                                 n_heads=N_HEADS, attn_drop_rate=0.1, layer_drop_rate=0.1, \n",
    "                                 dense_dim=DENSE_DIM).to(device)\n",
    "\n",
    "train_dl = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "dev_dl = DataLoader(val_data, batch_size=1, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use fast text word embedding\n",
    "EMBED_DIM = 300\n",
    "DENSE_DIM = 128\n",
    "\n",
    "model = FTTransformerClassfication(maxlen=5000, vocab_size=vocab_size, embed_dim=EMBED_DIM, \n",
    "                                 n_heads=N_HEADS, attn_drop_rate=0.1, layer_drop_rate=0.1, \n",
    "                                 dense_dim=DENSE_DIM).to(device)\n",
    "\n",
    "train_dl = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_ft)\n",
    "dev_dl = DataLoader(val_data, batch_size=1, shuffle=False, collate_fn=collate_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rEhD_Cg6bCz0",
    "outputId": "b8e4f007-e551-4199-bfd5-88a8a02e769f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    30/   94 batches | loss 0.662 | accuracy    0.658\n",
      "| epoch   1 |    60/   94 batches | loss 0.639 | accuracy    0.671\n",
      "| epoch   1 |    90/   94 batches | loss 0.639 | accuracy    0.671\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  2.15s | valid acc    0.653 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |    30/   94 batches | loss 0.655 | accuracy    0.666\n",
      "| epoch   2 |    60/   94 batches | loss 0.590 | accuracy    0.678\n",
      "| epoch   2 |    90/   94 batches | loss 0.641 | accuracy    0.674\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  2.13s | valid acc    0.665 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |    30/   94 batches | loss 0.631 | accuracy    0.680\n",
      "| epoch   3 |    60/   94 batches | loss 0.550 | accuracy    0.705\n",
      "| epoch   3 |    90/   94 batches | loss 0.636 | accuracy    0.711\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  2.11s | valid acc    0.670 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |    30/   94 batches | loss 0.560 | accuracy    0.718\n",
      "| epoch   4 |    60/   94 batches | loss 0.475 | accuracy    0.744\n",
      "| epoch   4 |    90/   94 batches | loss 0.615 | accuracy    0.738\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  2.11s | valid acc    0.663 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |    30/   94 batches | loss 0.471 | accuracy    0.770\n",
      "| epoch   5 |    60/   94 batches | loss 0.432 | accuracy    0.786\n",
      "| epoch   5 |    90/   94 batches | loss 0.561 | accuracy    0.789\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  2.11s | valid acc    0.648 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |    30/   94 batches | loss 0.463 | accuracy    0.785\n",
      "| epoch   6 |    60/   94 batches | loss 0.407 | accuracy    0.790\n",
      "| epoch   6 |    90/   94 batches | loss 0.520 | accuracy    0.797\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/SLURM_7892592/ipykernel_44219/1962194489.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0maccu_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_accu\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtotal_accu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0maccu_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/SLURM_7892592/ipykernel_44219/4128124402.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mpredicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mtotal_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/SLURM_7892592/ipykernel_44219/1019185506.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, seq_len)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    981\u001b[0m     return (_VF.dropout_(input, p, training)\n\u001b[1;32m    982\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m             else _VF.dropout(input, p, training))\n\u001b[0m\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/_VF.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dl, log_interval=30)\n",
    "    accu_val = evaluate(dev_dl)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid acc {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "TweetEval_fastText_demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py37_torch16_cuda102",
   "language": "python",
   "name": "proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
